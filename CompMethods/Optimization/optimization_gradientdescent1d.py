# -*- coding: utf-8 -*-
"""Optimization_GradientDescent1D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1equo-s-s0yyO7-PTS8ta3s65PRkyoGI_

Implementation of the Gradient Descent algorithm for a one-variable function
"""

import numpy as np
import matplotlib.pyplot as plt

def f(x):
  return x**4/16 -x**3/6 -5*x**2/8 +3*x/2 +3/4

npoints = 200

xvals = np.linspace(-4,4,npoints)
yvals = f(xvals)

plt.plot(xvals,yvals, linewidth = 2, label = 'f(x)')
plt.grid(True)
plt.xlabel('x')
plt.legend();

def df(x):
  return (x+2)*(x-1)*(x-3)/4

def gradient_descent(x0, l, tol, maxIterations):

  iterations = np.array([[0, x0, f(x0), np.abs(f(x0))]])
  n_iter = 1

  while iterations[-1,3] > tol and iterations[-1,0] < maxIterations:
    newx = iterations[-1,1] -l*df(iterations[-1,1]) #x_n = x_(n-1) -l*f'(x_(n-1))
    newf = f(newx)
    deltaf = np.abs(newf -iterations[-1,2])
    iterations = np.vstack([iterations, [n_iter, newx, newf, deltaf]])
    n_iter += 1

  print(iterations)

  return iterations

initialGuess = 0
learningRate = 0.05
tolerance = 0.001
maxIterations = 100

result = gradient_descent(initialGuess, learningRate, tolerance, maxIterations)

plt.figure(figsize=(10, 5))
plt.plot(xvals,yvals, linewidth = 2, label = 'f(x)')
plt.plot(result[:, 1], result[:, 2], marker='o', linestyle='-', \
         color='r',label = 'Gradient descent iterations')
plt.xlabel('x')
plt.grid(True)
plt.legend();

print("The global minimum of the function is", "{:.4f}".format(result[-1,2]), \
      "reached at x =", "{:.4f}".format(result[-1,1]))

from scipy.optimize import minimize

result_scipy = minimize(f, initialGuess)
result_scipy

xstar = result_scipy.x[0]
xstar

fxstar = result_scipy.fun
fxstar