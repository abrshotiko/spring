# -*- coding: utf-8 -*-
"""Optimization_GradientDescent3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aC4Z1vNByETvN1EoNDVA1G85W8PTQ683

Implementation of the Gradient Descent algorithm for a function of two variables
"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# First way to define a function of several variable: notice ho each variable
# is an independent input
def f3D(x, y):
    return (1/4) * x**4 - x**2 + x + 2*y**2 - 4*y + 2

# Generate x and y values
x = np.linspace(-3, 3, 100)
y = np.linspace(-1, 3, 100)

# Create a meshgrid for 3D plotting
X, Y = np.meshgrid(x, y)
Z = f3D(X, Y)

# Create a 3D plot
fig = plt.figure()
ax = plt.axes(projection='3d')

# Plot the surface
ax.plot_surface(X, Y, Z, alpha=0.8)

# Highlight axes
ax.plot([0, 0], [0, 0], [-3, 5], color='black', linestyle='dashed')
ax.plot([-3, 3], [0, 0], [0, 0], color='black', linestyle='dashed')
ax.plot([0, 0], [-1, 3], [0, 0], color='black', linestyle='dashed')

# Set labels
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x,y)')
ax.set_zlim(-3, 5)

# Show the plot
plt.show()

# Second way to define a function of several variable: each variable is an
# entry of the only (n-dimensial) input of the function.
# This method is recommended when the function plays the role of objective
# function of an optimization problem
def f3D(x):
    return (1/4) * x[0]**4 - x[0]**2 + x[0] + 2 * x[1]**2 - 4 * x[1] + 2

# Define the 2-dimensional gradient
def df3D(x):
    return np.array([x[0]**3 - 2 * x[0] + 1, 4 * x[1] - 4])

# Gradient Descent algorithm for 2-dimensional functions
def gradient_descent3D(x0, l, tol, maxIterations):
    n_iter = 0
    # Create an array to store iteration information
    iterations = np.array([[n_iter, x0[0], x0[1], f3D(x0), f3D(x0)]])
    n_iter = 1

    while iterations[-1, 4] > tol and iterations[-1, 0] < maxIterations:
        # Update the current position using the gradient
        newx = iterations[-1, 1:3] - l * df3D(iterations[-1, 1:3])
        newf = f3D(newx)
        # Calculate the change in the function value
        deltaf = np.abs(iterations[-1, 3] - newf)
        # Append the new iteration information to the array
        iterations = np.vstack([iterations, [n_iter, newx[0], newx[1], newf, deltaf]])
        n_iter += 1

    return iterations

# Initial parameters for the optimization
initialGuess3D = np.array([-2,-2])  # Initial guess for two variables
learningRate3D = 0.05
tolerance3D = 0.0001
maxIterations3D = 100

# Run the gradient descent algorithm
result3D = gradient_descent3D(initialGuess3D, learningRate3D, tolerance3D, maxIterations3D)

# Plotting
xvals3D = np.linspace(-3, 3, 100)
yvals3D = np.linspace(-3, 3, 100)

# Create a meshgrid for the contour plot
X, Y = np.meshgrid(xvals3D, yvals3D)
Z = (1/4) * X**4 - X**2 + X + 2 * Y**2 - 4 * Y + 2

# Plot the contour plot of the function and gradient descent iterations
plt.figure(figsize=(10, 5))
plt.contour(X, Y, Z, levels=20, cmap='viridis')  # Contour plot of the function
plt.plot(result3D[:, 1], result3D[:, 2], marker='o', linestyle='-', color='b', label='Gradient descent iterations')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.legend()

# Print the result
print("The global minimum of the function is", "{:.4f}".format(result3D[-1, 3]),
      "reached at x =", "{:.4f}".format(result3D[-1, 1]), "and y =", "{:.4f}".format(result3D[-1, 2]))

from scipy.optimize import minimize

result_scipy3D = minimize(f3D, initialGuess3D)

xstar3D = result_scipy3D.x
fxstar3D = result_scipy3D.fun

print("Optimal value of (x,y): (", "{:.4f}".format(xstar3D[0]),",","{:.4f}".format(xstar3D[1]),")")
print("Minimum value of the function:", "{:.4f}".format(fxstar3D))